{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers some of the details behind PCA. In particular this notebook covers the underlying math involved, and provides an implementation. If the linear algebra is a little confusing, feel free to contact me on Slack. Keep in mind, the linear algebra required for PCA will be covered in my notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "PCA is like any other machine learning algorithm, there is an associated cost/error function. Suppose that we have some data with $n$ features, and we have number of realisations of data given by $m$. This data would be stored as a ($m$ x $n$) matrix. The idea is to write each $n$ length piece of data as a linear combination of $k$ vectors. That way we only need to store the k vectors and the k coefficients for each data piece (Can be stored in a ($m$ x $k$) matrix.) We do this in a way such that the k vectors are chosen to minimise this cost function. Minimising this cost function, ensures that this smaller ($m$ x $k$) matrix, represents our data as closely as possible.\n",
    "\n",
    "Formally, If $x_1, x_2, ...., x_m$ is our data vectors, we want to find $k$ orthonormal vectors: $ \\{ v_1, v_2, ...., v_k \\} $ such that if $P$ denotes the orthogonal projection of vectors onto the space $V := Span \\{ v_1, v_2, ...., v_k \\}$, then the following cost function is minimised:\n",
    "\n",
    "$$ \\sum^{m}_{i=1} ||x_i - P(x_i)||^2 $$\n",
    "\n",
    "Assume we want to utilise $k$ components for our PCA algorithm. Recall that PCA is an approximation of our data. If we try to retrieve our data from the PCA matrix, this can be quite different to the original data matrix. Utilising more components in our algorithm will improve the accuracy of our approximation but consequently requires more work. \n",
    "\n",
    "Notation: \n",
    "- $X$ = Matrix of Data\n",
    "- $k$ = Number of Components used for PCA\n",
    "- $m$ = Number of data, ie, number of rows of $X$. \n",
    "\n",
    "\n",
    "\n",
    "There are 6 main steps in PCA(k - Components):\n",
    "\n",
    "1. Normalise your data, this isn't always necessary but provides an improvement in terms of optimisation. \n",
    "\n",
    "2. Compute the covariance matrix of the data.\n",
    "\n",
    " $ \\hspace{5cm} \\Sigma := \\frac{1}{m} X^{T}X $ = Covariance Matrix \n",
    "\n",
    "3. Compute eigenvalues of the covariance matrix, we need to sort the eigenvalues and take the $k$ largest eigenvalues and their corresponding eigenvectors. \n",
    "4. Form a new matrix (denote by $U_{reduced}$), with $k$-columns, each column given by an eigenvector, the eigenvectors should be arranged such that the eigenvectors with larger absolute values are positioned to the left of the matrix. \n",
    "\n",
    "Formally, let : $ e_1, e_2, e_3, .... e_k $ be the $k$ eigenvectors for the $k$ largest eigenvalues, then:\n",
    "\n",
    "$$ U_{reduced} = \\begin{pmatrix} e_1, e_2, e_3, ..... e_k \\end{pmatrix} \\in \\mathbb{R}^{m\\text{x}k} $$\n",
    "\n",
    "5. The columns of this new matrix is what forms our orthonomal basis of the subspace that we are projecting onto. The projection theorem states that, if P is the orthongal projection onto a $k$-dimensional subspace, where the projection distance is minimised, then we can write the projection of x as equation (1.1). \n",
    "\n",
    "$$ P(x) = \\sum^{k}_{i=1} \\langle x, e_i \\rangle e_i \\hspace{5cm} (1.1) $$\n",
    "\n",
    "6. Thus from equation (1.1) it follows that the $m$ x $k$ matrix or what I call the PCA matrix is given by: $U_{reduced}X  \\in \\mathbb{R}^{m\\text{x}k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sympy.interactive import printing \n",
    "printing.init_printing(use_latex = True)\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix):\n",
    "    \"\"\"\n",
    "    A function to normalise the data we have.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (numpy array): Matrix to be normalised.\n",
    "\n",
    "    Returns:\n",
    "        normalised_matrix (numpy array): normalised matrix\n",
    "        mean_vec (numpy array) : a numpy list of column means of the matrix provided.\n",
    "        std (numpy array) : a numpy list of column standard deviations of the matrix provided. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    number_of_data = matrix.shape[1]\n",
    "    mean_vec = matrix.mean(axis=0)\n",
    "    std = matrix.std(axis=0)\n",
    "    mean_matrix = np.einsum('ij,j-> ij', np.ones((number_of_data,number_of_data)), mean_vec)\n",
    "    normalised_matrix = matrix - mean_matrix \n",
    "    try:\n",
    "        normalised_matrix = np.einsum('ij, j-> ij', normalised_matrix, np.reciprocal(std))\n",
    "        return normalised_matrix, mean_vec, std  \n",
    "    except:\n",
    "        raise ValueError('Atleast one standard deviation is 0')\n",
    "    \n",
    "\n",
    "\n",
    "def pca(design_matrix, k):\n",
    "    \"\"\"\n",
    "    A function to apply the PCA algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        design_matrix (numpy array): A matrix of our data, should be normalised using the \"normalise\" function. \n",
    "        k (int): Integer representing the number of components we want to use. \n",
    "\n",
    "    Returns:\n",
    "        pca_matrix (numpy array): A matrix of coefficients representing our data.\n",
    "        k_components (numpy array): A matrix whose columns are the eigenvectors of the covariance matrix. \n",
    "        percent_retained (float): A float from 0-100, representing the variance of the data captured. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if k > design_matrix.shape[1]:\n",
    "        raise ValueError('The space you want to project to must have a lower dimension than the data')\n",
    "\n",
    "    else:\n",
    "        # Compute Covariance Matrix\n",
    "        m = design_matrix.shape[0]\n",
    "        covariance_matrix = 1/m * np.matmul(design_matrix.transpose(), design_matrix)\n",
    "\n",
    "\n",
    "        # Compute Eigenvalues, Eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "\n",
    "        idx_for_sort = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues_sorted = eigenvalues[idx_for_sort]\n",
    "        eigenvectors_sorted = eigenvectors[:,idx_for_sort]\n",
    "\n",
    "        # Take the first K-eigenvectors, these form an orthonormal basis of the Hyperplane we are projecting onto. \n",
    "        k_components = eigenvectors_sorted[:,:k]\n",
    "\n",
    "        pca_matrix = np.matmul(design_matrix, k_components)\n",
    "\n",
    "        percent_retained = eigenvalues_sorted[:k].sum()/eigenvalues_sorted.sum() * 100\n",
    "\n",
    "        return pca_matrix, k_components, percent_retained\n",
    "\n",
    "\n",
    "def retrieve_data(pca, components):\n",
    "    \"\"\"\n",
    "    Given our PCA matrix and the components, this function will retrieve an approximation of our original data. \n",
    "\n",
    "    Parameters:\n",
    "        pca (numpy array): Matrix of coefficients that represent our data.\n",
    "        components(numpy array): A matrix whose columns are the basis of the space we projected our data into. \n",
    "\n",
    "    Returns: \n",
    "        data (numpy array): An approximation of the original data. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data = np.matmul(pca, components.transpose())\n",
    "    return data \n",
    "    \n",
    "\n",
    "def un_normalise(normalised_matrix, mean, sd):\n",
    "    \"\"\"\n",
    "    A function to unormalise our data. \n",
    "\n",
    "    Parameters:\n",
    "        normalised_matrix (numpy array): Matrix to be un-normalised. \n",
    "        mean (numpy array): List of means we want un-normalise with.\n",
    "        sd (numpy array): list of standard deviations we want to un-normalise with. \n",
    "    \n",
    "    Returns:\n",
    "        un_normalised_data (numpy array): An unormalised matrix of data. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    number_of_data = normalised_matrix.shape[1]\n",
    "    mean_matrix = np.einsum('ij,j-> ij', np.ones((number_of_data, number_of_data)), mean)\n",
    "    un_normalised_data = np.einsum('ij, j->ij', normalised_matrix, sd) + mean_matrix \n",
    "    return approximate_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Data Compression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical example of PCA is data compression. PCA involves reducing the dimension of our $(m\\text{x}n)$ data matrix into a $(m\\text{x}k)$ matrix. We can store the data entirely as a $(m\\text{x}n)$ matrix, or we can store the $(m\\text{x}k)$ along with the $k$ eigenvectors, and when we need to use our data, we can retrive an approximation of our original data. \n",
    "\n",
    "### Retrieving Data:\n",
    "\n",
    "Notation: \n",
    "- $X_{approx}$ = Approximation of Data\n",
    "- $U_{reduced}$ = Same as above.\n",
    "- $Y$ = Output of PCA algorithm, ie, the $(m\\text{x}k)$ matrix. \n",
    "\n",
    "\n",
    "$$ X_{approx} = Y U_{reduced}^T $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to retrieve an approximation of our data. An application is shown below. We generate a random matrix with 5000 rows and 2000 columns and store as a CSV. Then we apply the PCA algorithm, store the PCA matrix and it's component matrix into a CSV file and compare the file sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage used for entire matrix: 250.0 Bytes\n",
      "Storage used for reduced matrix and matrix of eigenvectors: 107.1 Bytes\n"
     ]
    }
   ],
   "source": [
    "random_mat = np.random.random((5000,2000))\n",
    "np.savetxt('random_mat.csv', random_mat,delimiter=',')\n",
    "\n",
    "pca_data = pca(random_mat, 600)\n",
    "np.savetxt('PCA.csv', pca_data[0], delimiter=',')\n",
    "np.savetxt('K_Components.csv', pca_data[1], delimiter=',')\n",
    "\n",
    "retrieved_data = retrieve_data(pca_data[0], pca_data[1])\n",
    "\n",
    "print(f'Storage used for entire matrix: {round(os.path.getsize(\"./random_mat.csv\")/1000000,ndigits=2)} Bytes')\n",
    "print(f'Storage used for reduced matrix and matrix of eigenvectors: {round(os.path.getsize(\"./K_Components.csv\")/1000000 + os.path.getsize(\"./PCA.csv\")/1000000, ndigits=2)} Bytes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that storing the PCA matrix and the components matrix uses less than half the storage. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742eda1bdf1e218a3bb2b4bc9aaa5c450f2dcf1623d3e127dc8dee9e2156d7e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
